# TensorRT-LLM-Profiling
This repo demonstrates how to use Nsight Systems to profile a TensorRT-LLM model. In this particular example, Nsight Systems will profile an inference workload and renders utilization traces of various components of the GPU, host, and memory.

## Nsight Systems installation
Nsight Systems installation instruction may be found [here](https://docs.nvidia.com/nsight-systems/InstallationGuide/index.html).

## Inference script
Inference script for this example is [here](./speculative-decoding.py)
